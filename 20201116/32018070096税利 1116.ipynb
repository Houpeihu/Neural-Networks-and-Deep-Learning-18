{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.24386145547032356\n",
      "test_accuracy: 0.4666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 1, loss: 0.2066265232861042\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 2, loss: 0.19424713030457497\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 3, loss: 0.18378842249512672\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 4, loss: 0.17445169016718864\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 5, loss: 0.16637611761689186\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 6, loss: 0.15946495160460472\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 7, loss: 0.15355950593948364\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 8, loss: 0.14849310368299484\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 9, loss: 0.14411525800824165\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 10, loss: 0.14029918611049652\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 11, loss: 0.13694161176681519\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 12, loss: 0.13395968452095985\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 13, loss: 0.13128728792071342\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 14, loss: 0.12887168489396572\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 15, loss: 0.12667068652808666\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 16, loss: 0.1246503796428442\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 17, loss: 0.12278332561254501\n",
      "test_accuracy: 0.6333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 18, loss: 0.12104721367359161\n",
      "test_accuracy: 0.6666666666666666\n",
      "-------------------------------------------------\n",
      "epoch: 19, loss: 0.1194237619638443\n",
      "test_accuracy: 0.7\n",
      "-------------------------------------------------\n",
      "epoch: 20, loss: 0.11789790354669094\n",
      "test_accuracy: 0.7\n",
      "-------------------------------------------------\n",
      "epoch: 21, loss: 0.11645714938640594\n",
      "test_accuracy: 0.7\n",
      "-------------------------------------------------\n",
      "epoch: 22, loss: 0.11509108170866966\n",
      "test_accuracy: 0.7\n",
      "-------------------------------------------------\n",
      "epoch: 23, loss: 0.11379095166921616\n",
      "test_accuracy: 0.7\n",
      "-------------------------------------------------\n",
      "epoch: 24, loss: 0.11254936829209328\n",
      "test_accuracy: 0.7\n",
      "-------------------------------------------------\n",
      "epoch: 25, loss: 0.11136006936430931\n",
      "test_accuracy: 0.7\n",
      "-------------------------------------------------\n",
      "epoch: 26, loss: 0.11021770723164082\n",
      "test_accuracy: 0.7\n",
      "-------------------------------------------------\n",
      "epoch: 27, loss: 0.10911770537495613\n",
      "test_accuracy: 0.7333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 28, loss: 0.10805610753595829\n",
      "test_accuracy: 0.7666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 29, loss: 0.1070295087993145\n",
      "test_accuracy: 0.8\n",
      "-------------------------------------------------\n",
      "epoch: 30, loss: 0.10603493079543114\n",
      "test_accuracy: 0.8\n",
      "-------------------------------------------------\n",
      "epoch: 31, loss: 0.10506980121135712\n",
      "test_accuracy: 0.8333333333333334\n",
      "-------------------------------------------------\n",
      "epoch: 32, loss: 0.10413182526826859\n",
      "test_accuracy: 0.8333333333333334\n",
      "-------------------------------------------------\n",
      "epoch: 33, loss: 0.10321903973817825\n",
      "test_accuracy: 0.8333333333333334\n",
      "-------------------------------------------------\n",
      "epoch: 34, loss: 0.10232965648174286\n",
      "test_accuracy: 0.8333333333333334\n",
      "-------------------------------------------------\n",
      "epoch: 35, loss: 0.10146213322877884\n",
      "test_accuracy: 0.8333333333333334\n",
      "-------------------------------------------------\n",
      "epoch: 36, loss: 0.10061507672071457\n",
      "test_accuracy: 0.8333333333333334\n",
      "-------------------------------------------------\n",
      "epoch: 37, loss: 0.09978724271059036\n",
      "test_accuracy: 0.8666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 38, loss: 0.09897754155099392\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 39, loss: 0.09818496182560921\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 40, loss: 0.09740861877799034\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 41, loss: 0.09664770029485226\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 42, loss: 0.09590146318078041\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 43, loss: 0.09516925178468227\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 44, loss: 0.0944504626095295\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 45, loss: 0.09374452382326126\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 46, loss: 0.09305093437433243\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 47, loss: 0.09236922673881054\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 48, loss: 0.09169896692037582\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 49, loss: 0.09103975258767605\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 50, loss: 0.09039121866226196\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 51, loss: 0.08975301496684551\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 52, loss: 0.08912481740117073\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 53, loss: 0.08850633725523949\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 54, loss: 0.08789727836847305\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 55, loss: 0.08729738742113113\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 56, loss: 0.08670640736818314\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 57, loss: 0.086124112829566\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 58, loss: 0.08555028215050697\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 59, loss: 0.08498469740152359\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 60, loss: 0.08442717045545578\n",
      "test_accuracy: 0.9\n",
      "-------------------------------------------------\n",
      "epoch: 61, loss: 0.08387749642133713\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 62, loss: 0.0833355151116848\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 63, loss: 0.08280103653669357\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 64, loss: 0.08227390795946121\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 65, loss: 0.08175396174192429\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 66, loss: 0.08124106004834175\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 67, loss: 0.0807350380346179\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 68, loss: 0.08023577183485031\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 69, loss: 0.07974312361329794\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 70, loss: 0.07925696112215519\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n",
      "epoch: 71, loss: 0.07877716515213251\n",
      "test_accuracy: 0.9333333333333333\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 72, loss: 0.07830360159277916\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 73, loss: 0.07783616054803133\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 74, loss: 0.07737472653388977\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 75, loss: 0.07691919431090355\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 76, loss: 0.07646945025771856\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 77, loss: 0.07602540124207735\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 78, loss: 0.07558693457394838\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 79, loss: 0.0751539571210742\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 80, loss: 0.07472637295722961\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 81, loss: 0.07430408429354429\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 82, loss: 0.07388700917363167\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 83, loss: 0.0734750535339117\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 84, loss: 0.07306813634932041\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 85, loss: 0.07266616355627775\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 86, loss: 0.07226905506104231\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 87, loss: 0.07187673822045326\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 88, loss: 0.07148913200944662\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 89, loss: 0.07110615354031324\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 90, loss: 0.07072773016989231\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 91, loss: 0.07035379018634558\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 92, loss: 0.0699842693284154\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 93, loss: 0.06961908098310232\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 94, loss: 0.06925816554576159\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 95, loss: 0.06890144292265177\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 96, loss: 0.06854886934161186\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 97, loss: 0.06820037122815847\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 98, loss: 0.06785587500780821\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 99, loss: 0.06751532945781946\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 100, loss: 0.0671786805614829\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 101, loss: 0.06684585008770227\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 102, loss: 0.06651677656918764\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 103, loss: 0.06619142089039087\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 104, loss: 0.0658697159960866\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 105, loss: 0.06555161532014608\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 106, loss: 0.06523705366998911\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 107, loss: 0.06492598634213209\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 108, loss: 0.06461834814399481\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 109, loss: 0.06431409064680338\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 110, loss: 0.06401318870484829\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 111, loss: 0.06371556501835585\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 112, loss: 0.06342117488384247\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 113, loss: 0.06312997359782457\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 114, loss: 0.06284191645681858\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 115, loss: 0.06255696155130863\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 116, loss: 0.06227505672723055\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 117, loss: 0.061996158212423325\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 118, loss: 0.06172021944075823\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 119, loss: 0.06144719757139683\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 120, loss: 0.061177064664661884\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 121, loss: 0.06090976111590862\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 122, loss: 0.0606452627107501\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 123, loss: 0.06038351729512215\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 124, loss: 0.06012448575347662\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 125, loss: 0.059868134558200836\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 126, loss: 0.05961442366242409\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 127, loss: 0.059363314881920815\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 128, loss: 0.05911477282643318\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 129, loss: 0.05886876117438078\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 130, loss: 0.05862525478005409\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 131, loss: 0.05838420335203409\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 132, loss: 0.05814558081328869\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 133, loss: 0.05790934432297945\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 134, loss: 0.05767547059804201\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 135, loss: 0.057443926110863686\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 136, loss: 0.057214670814573765\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 137, loss: 0.056987689808011055\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 138, loss: 0.056762934662401676\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 139, loss: 0.05654038768261671\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 140, loss: 0.0563200106844306\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 141, loss: 0.05610177293419838\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 142, loss: 0.055885656736791134\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 143, loss: 0.0556716239079833\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 144, loss: 0.0554596446454525\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 145, loss: 0.05524970591068268\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 146, loss: 0.05504176765680313\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 147, loss: 0.054835802875459194\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 148, loss: 0.054631791077554226\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 149, loss: 0.05442970246076584\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 150, loss: 0.054229521192610264\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 151, loss: 0.05403121467679739\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 152, loss: 0.05383475590497255\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 153, loss: 0.05364011786878109\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 154, loss: 0.053447285667061806\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 155, loss: 0.05325623322278261\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 156, loss: 0.05306693725287914\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 157, loss: 0.05287937540560961\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 158, loss: 0.05269352998584509\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 159, loss: 0.05250936932861805\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 160, loss: 0.05232687387615442\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 161, loss: 0.052146025002002716\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 162, loss: 0.05196680873632431\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 163, loss: 0.05178919807076454\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 164, loss: 0.05161316134035587\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 165, loss: 0.05143869947642088\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 166, loss: 0.05126578453928232\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 167, loss: 0.05109438579529524\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 168, loss: 0.050924502313137054\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 169, loss: 0.05075610335916281\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 170, loss: 0.05058917589485645\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 171, loss: 0.05042370827868581\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 172, loss: 0.050259662326425314\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 173, loss: 0.050097044091671705\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 174, loss: 0.04993581539019942\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 175, loss: 0.049775978084653616\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 176, loss: 0.0496174986474216\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 177, loss: 0.04946037055924535\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 178, loss: 0.04930457333102822\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 179, loss: 0.04915009066462517\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 180, loss: 0.048996909987181425\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 181, loss: 0.04884502151980996\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 182, loss: 0.048694401048123837\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 183, loss: 0.04854502948001027\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 184, loss: 0.048396897967904806\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 185, loss: 0.04824998835101724\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 186, loss: 0.04810430109500885\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 187, loss: 0.047959806863218546\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 188, loss: 0.04781649028882384\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 189, loss: 0.04767433600500226\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 190, loss: 0.047533347737044096\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 191, loss: 0.04739349661394954\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 192, loss: 0.04725477332249284\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 193, loss: 0.04711716575548053\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 194, loss: 0.04698066459968686\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 195, loss: 0.04684525262564421\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 196, loss: 0.04671091353520751\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 197, loss: 0.04657763848081231\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 198, loss: 0.046445414423942566\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 199, loss: 0.046314232517033815\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 200, loss: 0.0461840913631022\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 201, loss: 0.046054957900196314\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 202, loss: 0.045926826540380716\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 203, loss: 0.04579969355836511\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 204, loss: 0.04567353893071413\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 205, loss: 0.045548363123089075\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 206, loss: 0.04542414518073201\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 207, loss: 0.04530087858438492\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 208, loss: 0.0451785558834672\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 209, loss: 0.04505715845152736\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 210, loss: 0.04493668070062995\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 211, loss: 0.044817115645855665\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 212, loss: 0.04469844698905945\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 213, loss: 0.044580668676644564\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 214, loss: 0.04446378070861101\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 215, loss: 0.04434775235131383\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 216, loss: 0.04423259850591421\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 217, loss: 0.04411828750744462\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 218, loss: 0.044004823081195354\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 219, loss: 0.043892189394682646\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 220, loss: 0.043780375737696886\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 221, loss: 0.043669382110238075\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 222, loss: 0.04355920339003205\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 223, loss: 0.043449809309095144\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 224, loss: 0.04334121895954013\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 225, loss: 0.043233399745076895\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 226, loss: 0.043126367032527924\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 227, loss: 0.043020089622586966\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 228, loss: 0.04291458101943135\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 229, loss: 0.04280980536714196\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 230, loss: 0.04270578408613801\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 231, loss: 0.04260250134393573\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 232, loss: 0.042499938514083624\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 233, loss: 0.042398094199597836\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 234, loss: 0.04229696234688163\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 235, loss: 0.042196537367999554\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 236, loss: 0.0420968122780323\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 237, loss: 0.04199777403846383\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 238, loss: 0.04189942218363285\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 239, loss: 0.04180174274370074\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 240, loss: 0.04170473385602236\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 241, loss: 0.0416083917953074\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 242, loss: 0.04151270445436239\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 243, loss: 0.04141766997054219\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 244, loss: 0.04132327111437917\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 245, loss: 0.04122951300814748\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 246, loss: 0.04113638726994395\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 247, loss: 0.04104388505220413\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 248, loss: 0.04095200588926673\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 249, loss: 0.0408607367426157\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 250, loss: 0.04077006317675114\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 251, loss: 0.040680000092834234\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 252, loss: 0.040590533055365086\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 253, loss: 0.04050164809450507\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 254, loss: 0.04041334008798003\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 255, loss: 0.0403256262652576\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 256, loss: 0.04023847170174122\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 257, loss: 0.04015188571065664\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 258, loss: 0.040065857116132975\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 259, loss: 0.03998039895668626\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 260, loss: 0.03989547677338123\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 261, loss: 0.03981110779568553\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 262, loss: 0.03972726268693805\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 263, loss: 0.039643969386816025\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 264, loss: 0.03956119390204549\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 265, loss: 0.039478946942836046\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 266, loss: 0.0393972210586071\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 267, loss: 0.039316011127084494\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 268, loss: 0.03923530504107475\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 269, loss: 0.0391551093198359\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 270, loss: 0.039075412787497044\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 271, loss: 0.03899621171876788\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 272, loss: 0.03891750751063228\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 273, loss: 0.038839280139654875\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 274, loss: 0.03876154124736786\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 275, loss: 0.03868427826091647\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 276, loss: 0.03860749490559101\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 277, loss: 0.038531165570020676\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 278, loss: 0.038455317728221416\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 279, loss: 0.038379916455596685\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 280, loss: 0.03830498503521085\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 281, loss: 0.038230502512305975\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 282, loss: 0.03815646609291434\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 283, loss: 0.038082870189100504\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 284, loss: 0.0380097096785903\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 285, loss: 0.03793699759989977\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 286, loss: 0.03786470787599683\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 287, loss: 0.03779285866767168\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 288, loss: 0.037721427623182535\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 289, loss: 0.03765042498707771\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 290, loss: 0.03757983073592186\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 291, loss: 0.03750965418294072\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 292, loss: 0.03743987949565053\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 293, loss: 0.03737052297219634\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 294, loss: 0.03730155760422349\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 295, loss: 0.03723300201818347\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 296, loss: 0.037164832931011915\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 297, loss: 0.03709706291556358\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 298, loss: 0.03702968405559659\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 299, loss: 0.03696268564090133\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 300, loss: 0.036896072793751955\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 301, loss: 0.03682984225451946\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 302, loss: 0.03676398238167167\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 303, loss: 0.03669849969446659\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 304, loss: 0.036633382085710764\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 305, loss: 0.0365686290897429\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 306, loss: 0.036504244431853294\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 307, loss: 0.03644021600484848\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 308, loss: 0.03637655172497034\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 309, loss: 0.03631323669105768\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 310, loss: 0.03625027136877179\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 311, loss: 0.03618765436112881\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 312, loss: 0.03612538939341903\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 313, loss: 0.036063462030142546\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 314, loss: 0.036001880653202534\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 315, loss: 0.035940627101808786\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 316, loss: 0.03587970929220319\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 317, loss: 0.035819131415337324\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 318, loss: 0.03575887205079198\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 319, loss: 0.03569894563406706\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 320, loss: 0.035639338195323944\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 321, loss: 0.03558005392551422\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 322, loss: 0.035521088633686304\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 323, loss: 0.035462434869259596\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 324, loss: 0.035404101479798555\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 325, loss: 0.03534606425091624\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 326, loss: 0.03528835019096732\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 327, loss: 0.0352309406735003\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 328, loss: 0.03517382824793458\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 329, loss: 0.03511701477691531\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 330, loss: 0.03506050491705537\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 331, loss: 0.035004285629838705\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 332, loss: 0.03494836622849107\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 333, loss: 0.03489273367449641\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 334, loss: 0.034837395418435335\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 335, loss: 0.03478233469650149\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 336, loss: 0.034727565478533506\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 337, loss: 0.03467307658866048\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 338, loss: 0.0346188684925437\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 339, loss: 0.0345649397931993\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 340, loss: 0.03451127838343382\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 341, loss: 0.03445789264515042\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 342, loss: 0.03440478350967169\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 343, loss: 0.034351945389062166\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 344, loss: 0.034299371764063835\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 345, loss: 0.034247060772031546\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 346, loss: 0.03419501380994916\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 347, loss: 0.034143227618187666\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 348, loss: 0.03409169893711805\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 349, loss: 0.03404043149203062\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 350, loss: 0.03398942807689309\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 351, loss: 0.03393866354599595\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 352, loss: 0.03388815512880683\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 353, loss: 0.03383789863437414\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 354, loss: 0.03378788474947214\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 355, loss: 0.033738120924681425\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 356, loss: 0.03368859784677625\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 357, loss: 0.033639322966337204\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 358, loss: 0.03359027998521924\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 359, loss: 0.033541475888341665\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 360, loss: 0.03349291440099478\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 361, loss: 0.033444583881646395\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 362, loss: 0.03339648945257068\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 363, loss: 0.03334862529300153\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 364, loss: 0.03330099186860025\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 365, loss: 0.03325359057635069\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 366, loss: 0.03320640604943037\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 367, loss: 0.033159453654661775\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 368, loss: 0.033112715696915984\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 369, loss: 0.03306620544753969\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 370, loss: 0.033019914990291\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 371, loss: 0.032973840134218335\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 372, loss: 0.03292798902839422\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 373, loss: 0.03288235515356064\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 374, loss: 0.03283692733384669\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 375, loss: 0.03279170976020396\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 376, loss: 0.03274670708924532\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 377, loss: 0.032701913034543395\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 378, loss: 0.032657323172315955\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 379, loss: 0.03261294378899038\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 380, loss: 0.03256877395324409\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 381, loss: 0.032524799229577184\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 382, loss: 0.03248102799989283\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 383, loss: 0.03243746259249747\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 384, loss: 0.03239409136585891\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 385, loss: 0.03235091897659004\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 386, loss: 0.03230794216506183\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 387, loss: 0.03226516116410494\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 388, loss: 0.032222570618614554\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 389, loss: 0.03218017867766321\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 390, loss: 0.03213797160424292\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 391, loss: 0.032095960108563304\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 392, loss: 0.032054132549092174\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 393, loss: 0.032012491254135966\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 394, loss: 0.03197102923877537\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 395, loss: 0.03192976303398609\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 396, loss: 0.03188867517746985\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 397, loss: 0.031847769394516945\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 398, loss: 0.03180704661644995\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 399, loss: 0.031766501255333424\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 400, loss: 0.031726131215691566\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 401, loss: 0.03168595093302429\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 402, loss: 0.03164593153633177\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 403, loss: 0.03160609118640423\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 404, loss: 0.031566431978717446\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 405, loss: 0.031526930863037705\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 406, loss: 0.03148761298507452\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 407, loss: 0.031448457622900605\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 408, loss: 0.031409477815032005\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 409, loss: 0.031370664946734905\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 410, loss: 0.031332012033089995\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 411, loss: 0.031293523497879505\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 412, loss: 0.03125521098263562\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 413, loss: 0.031217056093737483\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 414, loss: 0.03117905673570931\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 415, loss: 0.031141228042542934\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 416, loss: 0.03110355301760137\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 417, loss: 0.03106604120694101\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 418, loss: 0.031028683995828032\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 419, loss: 0.030991486040875316\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 420, loss: 0.030954445712268353\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 421, loss: 0.030917558586224914\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 422, loss: 0.03088082862086594\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 423, loss: 0.030844253487885\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 424, loss: 0.03080782969482243\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 425, loss: 0.0307715458329767\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 426, loss: 0.030735419131815434\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 427, loss: 0.030699441907927394\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 428, loss: 0.030663617188110948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 429, loss: 0.030627933330833912\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 430, loss: 0.030592398950830102\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 431, loss: 0.030557010555639863\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 432, loss: 0.030521762324497104\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 433, loss: 0.030486657517030835\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 434, loss: 0.030451703583821654\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 435, loss: 0.030416883528232574\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 436, loss: 0.030382206663489342\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 437, loss: 0.0303476732224226\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 438, loss: 0.03031328064389527\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 439, loss: 0.030279013561084867\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 440, loss: 0.030244892230257392\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 441, loss: 0.03021090547554195\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 442, loss: 0.030177062144502997\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 443, loss: 0.030143347568809986\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 444, loss: 0.030109765706583858\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 445, loss: 0.030076325638219714\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 446, loss: 0.030043008737266064\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 447, loss: 0.030009818961843848\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 448, loss: 0.02997677307575941\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 449, loss: 0.02994384616613388\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 450, loss: 0.029911059187725186\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 451, loss: 0.029878386994823813\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 452, loss: 0.029845853336155415\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 453, loss: 0.029813436791300774\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 454, loss: 0.029781155986711383\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 455, loss: 0.029748998815193772\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 456, loss: 0.02971696318127215\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 457, loss: 0.02968505653552711\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 458, loss: 0.02965326141566038\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 459, loss: 0.0296216057613492\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 460, loss: 0.02959006209857762\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 461, loss: 0.0295586371794343\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 462, loss: 0.029527338920161128\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 463, loss: 0.029496153816580772\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 464, loss: 0.029465095372870564\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 465, loss: 0.029434140073135495\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 466, loss: 0.02940330933779478\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 467, loss: 0.029372606659308076\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 468, loss: 0.029342004796490073\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 469, loss: 0.029311532387509942\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 470, loss: 0.029281155206263065\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 471, loss: 0.029250902822241187\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 472, loss: 0.02922076848335564\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 473, loss: 0.029190736124292016\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 474, loss: 0.029160834616050124\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 475, loss: 0.02913102600723505\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 476, loss: 0.029101330554112792\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 477, loss: 0.02907175081782043\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 478, loss: 0.029042272130027413\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 479, loss: 0.02901291218586266\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 480, loss: 0.02898366004228592\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 481, loss: 0.028954506618902087\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 482, loss: 0.028925468446686864\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 483, loss: 0.028896534582599998\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 484, loss: 0.02886770968325436\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 485, loss: 0.028838977916166186\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 486, loss: 0.028810354648157954\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 487, loss: 0.0287818422075361\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 488, loss: 0.02875343174673617\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 489, loss: 0.028725121170282364\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 490, loss: 0.028696909314021468\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 491, loss: 0.02866880944930017\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 492, loss: 0.028640802018344402\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 493, loss: 0.028612902155146003\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 494, loss: 0.028585095889866352\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 495, loss: 0.028557382756844163\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 496, loss: 0.028529785806313157\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 497, loss: 0.028502274537459016\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 498, loss: 0.028474862687289715\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n",
      "epoch: 499, loss: 0.02844755514524877\n",
      "test_accuracy: 0.9666666666666667\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from sklearn import datasets  \n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    " \n",
    "#导入数据\n",
    "x_data = datasets.load_iris().data #导入iris数据集的特征\n",
    " \n",
    "y_data = datasets.load_iris().target #导入iris数据集的标签\n",
    " \n",
    "#随机打乱顺序，使训练更具准确性\n",
    "np.random.seed(120)#调用numpy中的random方法里的seed方法，赋值120，使输入特征和标签能够一一对应\n",
    " \n",
    "np.random.shuffle(x_data) #调用numpy中的random方法里的shuffle方法，将训练集x_data里的特征值乱序\n",
    " \n",
    "np.random.seed(120)#调用numpy中的random方法里的seed方法，赋值120，使输入特征和标签能够一一对应\n",
    " \n",
    "np.random.shuffle(y_data) #调用numpy中的random方法里的shuffle方法，将测试集y_data里的标签乱序\n",
    " \n",
    "tf.random.set_seed(120)#调用tensorflow中的random方法里的set_seed方法，赋值120\n",
    " \n",
    "#划分数据集\n",
    "x_train = x_data[:-30] #将iris数据集（特征，共150行，此时已打乱）前120行作为训练集x_train\n",
    " \n",
    "y_train = y_data[:-30] #将iris数据集（标签，共150行，此时已打乱）前120行作为训练集y_train\n",
    " \n",
    "x_test = x_data[-30:] #将iris数据集（特征，共150行，此时已打乱）最后30行作为测试集x_test\n",
    " \n",
    "y_test = y_data[-30:] #将iris数据集（标签，共150行，此时已打乱）最后30行作为测试集y_test\n",
    " \n",
    "#转换特征值的数据类型，使之与后面数据运算时数据类型一致\n",
    "x_train = tf.cast(x_train, dtype = tf.float32) #调用tensorflow中的cast方法，将x_train中的特征值类型转换为float32\n",
    " \n",
    "x_test = tf.cast(x_test, dtype = tf.float32) #调用tensorflow中的cast方法，将x_test中的特征值类型转换为float32\n",
    " \n",
    "#用from_tensor_slices方法将特征值和标签值配对\n",
    "train_data_batch = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)#将训练集的特征x_train和标签y_train配对，用batch方法将120个训练数据分成32个为一组的批次\n",
    " \n",
    "test_data_batch = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)#将测试集的特征x_test和标签y_test配对，用batch方法将30个训练数据分成32个为一组的批次\n",
    " \n",
    "#用truncated_normal方法构建神经网络，并用Variable方法标记可训练数据\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev = 0.1, seed = 1))#用truncated_normal方法,构建4个输入特征，3个分类的神经网络结构，标准差为0.1的正态分布，随机种子为1\n",
    " \n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev = 0.1, seed = 1))#用truncated_normal方法,因为b1和w1的分类维度要一样，所以是3，标准差为0.1的正态分布，随机种子为1\n",
    " \n",
    "#设置变量\n",
    "learnRate = 0.1 #学习率为0.1\n",
    " \n",
    "train_loss_results = [] #将每轮的loss记录在此列表中，为后面画loss曲线时提供数据\n",
    " \n",
    "test_accuracy = [] #将每轮的精度accuracy记录在此列表中，为后面画精度accuracy曲线提供数据\n",
    " \n",
    "epoch = 500 #循环500轮\n",
    " \n",
    "loss_all = 0 #每轮分4个step,loss_all记录4个step生成的4个loss的和\n",
    " \n",
    "#训练部分\n",
    "for epoch in range(epoch): #遍历数据集，每个epoch循环一次数据集\n",
    "    for step, (x_train, y_train) in enumerate(train_data_batch): #遍历batch，每个step循环一次batch\n",
    "        with tf.GradientTape() as tape: #用上下文管理器记录梯度信息\n",
    "            y = tf.matmul(x_train, w1) + b1 #神经网络乘加运算，用tensorflow中的matmul方法将训练特征值x_train和w1参数进行矩阵相乘\n",
    "            y = tf.nn.softmax(y) #用tensorflow中的softmax方法将神经网络乘加运算后得到的输出符合正态分布，输出和为1，可以在之后用来与独热码相减求loss\n",
    "            y_one_hot = tf.one_hot(y_train, depth = 3) #用tensorflow中的one_hot方法将训练标签y_train转换为独热码格式，因为y输出为3，所以深度为3，方便接下来计算loss的和\n",
    "            loss = tf.reduce_mean(tf.square(y_one_hot - y)) #用tensorflow中的reduce_mean方法求平均值，用tensorflow中的square方法求平方，这里用均方误差求损失函数loss\n",
    "            loss_all += loss.numpy() #将每个step计算出的loss累加，后面可以用来求loss平均值，\n",
    " \n",
    "        #计算loss对各个参数的梯度\n",
    "        loss_gradient = tape.gradient(loss, [w1, b1])#用tensorflow中的GradientTape方法中的gradient方法求loss对各个参数w1,b1的梯度gradient\n",
    " \n",
    "        #梯度更新\n",
    "        w1.assign_sub(learnRate * loss_gradient[0]) #用assign_sub方法进行自减，实现参数w1的自动更新,等价于w1 = w1 - learn_Rate * loss_gradient[0]\n",
    "        b1.assign_sub(learnRate * loss_gradient[1]) #用assign_sub方法进行自减，实现参数b1的自动更新,等价于b = b - learn_Rate * loss_gradient[1]\n",
    " \n",
    "    # 每个epoch,打印loss信息\n",
    "    print(\"epoch: {}, loss: {}\".format(epoch,\n",
    "                                       loss_all / 4))  # 每个epoch,打印loss信息,有4个step，所以总loss_all要除以4，求得每次step的平均loss\n",
    "    train_loss_results.append(loss_all / 4)  # 用append方法将4个step的loss求平均值记录在train_loss_results中\n",
    "    loss_all = 0  # loss_all归零，为下一个epoch的求loss做准备\n",
    " \n",
    "    # 测试部分\n",
    "    total_correct = 0  # total_correct为预测对的样本个数，初始化为0\n",
    "    total_test_number = 0  # total_number为测试的总样本数，初始化为0\n",
    " \n",
    "    for x_test, y_test in test_data_batch:  # 遍历训练集的特征值和标签值\n",
    "        # 用更新后的参数进行预测\n",
    "        y = tf.matmul(x_test, w1) + b1  # 用tensorflow中的matmul方法来进行乘加运算，再加上b1得到前向传播的结果\n",
    "        y = tf.nn.softmax(y)  # 用tensorflow中的softmax方法将神经网络乘加运算后得到的前向传播的结果符合正态分布，输出和为1，可以在之后用来与独热码相减求loss\n",
    "        predict = tf.argmax(y, axis=1)  # 用tensorflow中的argmax方法，返回y中最大值的索引，即预测的标签分类，axis表示按列求值\n",
    "        predict = tf.cast(predict, dtype=y_test.dtype)  # 将predict的类型转换为测试集标签y_test的数据类型\n",
    "        correct = tf.cast(tf.equal(predict, y_test),\n",
    "                          dtype=tf.int32)  # 用tensorflow中的equal方法判断，若分类正确，则值为1，否则为0，并用tensorflow中的cast方法将bool类型转化为int32类型\n",
    "        correct = tf.reduce_sum(correct)  # 用tensorflow中的reduce_sum方法将每个batch的correct数加起来\n",
    "        total_correct += int(correct)  # 将所有batch中的correct数转化为int类型，并加起来\n",
    "        total_test_number += x_test.shape[0]  # 用shape方法返回测试集特征x_test的行数，也就是测试的总样本数\n",
    " \n",
    "    accuracy = total_correct / total_test_number  # 总的准确率\n",
    "    test_accuracy.append(accuracy)  # 测试集的准确率添加到列表中来，方便记录\n",
    "    print(\"test_accuracy:\", accuracy)  # 打印测试集精度准确率\n",
    "    print(\"-------------------------------------------------\")  # 为每个epoch进行分隔，方便查看\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
